#这个是从原来项目的readme中摘出来的 对我非常有用 无论是理解还是帮助以后的测试 所以需要马上搞起来

Percona XtraDB Cluster Tutorial
================================

Ways to test the Cluster
------------------------

Running pt-heartbeat
~~~~~~~~~~~~~~~~~~~~

I use pt-heartbeat in my PXC testing to show when there are replication hiccups and delays.  Due to a limitation of pt-heartbeat, we must create a legacy version of the heartbeat table that will work with PXC::

	node2 mysql> create schema percona;
	Query OK, 1 row affected (0.00 sec)

	node2 mysql> CREATE TABLE percona.heartbeat (
	    id int NOT NULL PRIMARY KEY,
	    ts datetime NOT NULL
	    );
	Query OK, 0 rows affected (0.01 sec)

Now, start pt-heartbeat on node2::

	[root@node2 ~]# pt-heartbeat --update --database percona

One node1, let's monitor the heartbeat::

	[root@node1 ~]# pt-heartbeat --monitor --database percona
	   0s [  0.00s,  0.00s,  0.00s ]
	   0s [  0.00s,  0.00s,  0.00s ]
	   0s [  0.00s,  0.00s,  0.00s ]
	   0s [  0.00s,  0.00s,  0.00s ]
	   0s [  0.00s,  0.00s,  0.00s ]
	   0s [  0.00s,  0.00s,  0.00s ]
	   0s [  0.00s,  0.00s,  0.00s ]
	   0s [  0.00s,  0.00s,  0.00s ]

This output will show us if there are any delays in the heartbeat compared with the current time.  


Monitoring commit latency
~~~~~~~~~~~~~~~~~~~~~~~~~~

To illustrate high client write latency, I have created a script called ``quick_update.pl``, which should be in your path.  This script does the following:
	- Runs the same UPDATE command that pt-heartbeat does, though with only 10ms of sleep between each execution. It updates and prints a counter on each execution. 
	- If it detects any of the UPDATEs took more than 50ms (this is configurable if you edit the script), then it prints 'slow', the date timestamp, and the final query latency is printed (in seconds) when the query does finish.  

If you haven't done so yet, create the ``percona`` schema and the ``heartbeat`` table as per the last section::  

	node2 mysql> create schema percona;
	use percona;
	CREATE TABLE heartbeat (
		id int NOT NULL PRIMARY KEY,
		ts datetime NOT NULL
	);
	insert into heartbeat (id, ts) values (1, NOW());

The execution looks something like::

	[root@node1 ~]# quick_update.pl 
	9886
	slow: Wed Aug 15 15:01:19 CEST 2012 0.139s
	10428

Note that occasionally the writes to the 3 node cluster setup on VMs on your laptop might be sporadically slow. This can be taken as noise.  


Using sysbench to generate load
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To simulate a live environment, we will kick off setup and kickoff a sysbench oltp test with a single test thread.

**Prepare the test table**

::

[root@node1 ~]# sysbench --test=sysbench_tests/db/common.lua --mysql-user=root --mysql-db=test --oltp-table-size=250000 prepare


**Start a Test run**

::

	[root@node1 ~]# sysbench --test=sysbench_tests/db/oltp.lua --mysql-user=root --mysql-db=test --oltp-table-size=250000 --report-interval=1 --max-requests=0 --tx-rate=10 run | grep tps
	[   1s] threads: 1, tps: 11.00, reads/s: 154.06, writes/s: 44.02, response time: 41.91ms (95%)
	[   2s] threads: 1, tps: 18.00, reads/s: 252.03, writes/s: 72.01, response time: 24.02ms (95%)
	[   3s] threads: 1, tps: 9.00, reads/s: 126.01, writes/s: 36.00, response time: 20.74ms (95%)
	[   4s] threads: 1, tps: 13.00, reads/s: 181.97, writes/s: 51.99, response time: 19.19ms (95%)
	[   5s] threads: 1, tps: 13.00, reads/s: 182.00, writes/s: 52.00, response time: 22.75ms (95%)
	[   6s] threads: 1, tps: 10.00, reads/s: 140.00, writes/s: 40.00, response time: 22.35ms (95%)
	[   7s] threads: 1, tps: 13.00, reads/s: 181.99, writes/s: 52.00, response time: 21.09ms (95%)
	[   8s] threads: 1, tps: 13.00, reads/s: 181.99, writes/s: 52.00, response time: 23.71ms (95%)

Your performance may vary.  Note we are setting ``--tx-rate`` as a way to prevent your VMs from working too hard.  Feel free to adjust ``-tx-rate`` accordingly, but be sure that you have several operations a second for the following tests.

As the WARNING message indicates, this test will go forever until you ``Ctrl-C`` it.  You can kill and restart this test at any time

**Cleanup test table**

Note that if you mess something up, you can cleanup the test table and start these steps over if needed::

	[root@node1 ~]# sysbench --test=sysbench_tests/db/common.lua --mysql-user=root --mysql-db=test cleanup
	sysbench 0.5:  multi-threaded system evaluation benchmark

	Dropping table 'sbtest1'...
